/* tslint:disable */
/* eslint-disable */
/**
 * AI Stats Gateway API
 * Programmatic access to the AI Stats AI Gateway. All endpoints forward requests to the best available provider for the specified model, and return normalised responses with clear structured metadata.
 *
 * The version of the OpenAPI document: 0.1.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import { mapValues } from '../runtime';
import type { ChatCompletionsRequestToolChoice } from './ChatCompletionsRequestToolChoice';
import {
    ChatCompletionsRequestToolChoiceFromJSON,
    ChatCompletionsRequestToolChoiceFromJSONTyped,
    ChatCompletionsRequestToolChoiceToJSON,
    ChatCompletionsRequestToolChoiceToJSONTyped,
} from './ChatCompletionsRequestToolChoice';
import type { ChatMessage } from './ChatMessage';
import {
    ChatMessageFromJSON,
    ChatMessageFromJSONTyped,
    ChatMessageToJSON,
    ChatMessageToJSONTyped,
} from './ChatMessage';
import type { ChatCompletionsRequestReasoning } from './ChatCompletionsRequestReasoning';
import {
    ChatCompletionsRequestReasoningFromJSON,
    ChatCompletionsRequestReasoningFromJSONTyped,
    ChatCompletionsRequestReasoningToJSON,
    ChatCompletionsRequestReasoningToJSONTyped,
} from './ChatCompletionsRequestReasoning';

/**
 * 
 * @export
 * @interface ChatCompletionsRequest
 */
export interface ChatCompletionsRequest {
    /**
     * 
     * @type {ChatCompletionsRequestReasoning}
     * @memberof ChatCompletionsRequest
     */
    reasoning?: ChatCompletionsRequestReasoning;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    frequency_penalty?: number;
    /**
     * 
     * @type {{ [key: string]: number; }}
     * @memberof ChatCompletionsRequest
     */
    logit_bias?: { [key: string]: number; };
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    max_output_tokens?: number;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    max_completions_tokens?: number;
    /**
     * Include gateway metadata in the response.
     * @type {boolean}
     * @memberof ChatCompletionsRequest
     */
    meta?: boolean;
    /**
     * 
     * @type {string}
     * @memberof ChatCompletionsRequest
     */
    model: string;
    /**
     * 
     * @type {Array<ChatMessage>}
     * @memberof ChatCompletionsRequest
     */
    messages: Array<ChatMessage>;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    presence_penalty?: number;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    seed?: number;
    /**
     * 
     * @type {boolean}
     * @memberof ChatCompletionsRequest
     */
    stream?: boolean;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    temperature?: number;
    /**
     * 
     * @type {Array<object>}
     * @memberof ChatCompletionsRequest
     */
    tools?: Array<object>;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    max_tool_calls?: number;
    /**
     * 
     * @type {boolean}
     * @memberof ChatCompletionsRequest
     */
    parallel_tool_calls?: boolean;
    /**
     * 
     * @type {ChatCompletionsRequestToolChoice}
     * @memberof ChatCompletionsRequest
     */
    tool_choice?: ChatCompletionsRequestToolChoice;
    /**
     * 
     * @type {boolean}
     * @memberof ChatCompletionsRequest
     */
    logprobs?: boolean;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    top_logprobs?: number;
    /**
     * 
     * @type {number}
     * @memberof ChatCompletionsRequest
     */
    top_p?: number;
    /**
     * Include token usage details in the response.
     * @type {boolean}
     * @memberof ChatCompletionsRequest
     */
    usage?: boolean;
}

/**
 * Check if a given object implements the ChatCompletionsRequest interface.
 */
export function instanceOfChatCompletionsRequest(value: object): value is ChatCompletionsRequest {
    if (!('model' in value) || value['model'] === undefined) return false;
    if (!('messages' in value) || value['messages'] === undefined) return false;
    return true;
}

export function ChatCompletionsRequestFromJSON(json: any): ChatCompletionsRequest {
    return ChatCompletionsRequestFromJSONTyped(json, false);
}

export function ChatCompletionsRequestFromJSONTyped(json: any, ignoreDiscriminator: boolean): ChatCompletionsRequest {
    if (json == null) {
        return json;
    }
    return {
        
        'reasoning': json['reasoning'] == null ? undefined : ChatCompletionsRequestReasoningFromJSON(json['reasoning']),
        'frequency_penalty': json['frequency_penalty'] == null ? undefined : json['frequency_penalty'],
        'logit_bias': json['logit_bias'] == null ? undefined : json['logit_bias'],
        'max_output_tokens': json['max_output_tokens'] == null ? undefined : json['max_output_tokens'],
        'max_completions_tokens': json['max_completions_tokens'] == null ? undefined : json['max_completions_tokens'],
        'meta': json['meta'] == null ? undefined : json['meta'],
        'model': json['model'],
        'messages': ((json['messages'] as Array<any>).map(ChatMessageFromJSON)),
        'presence_penalty': json['presence_penalty'] == null ? undefined : json['presence_penalty'],
        'seed': json['seed'] == null ? undefined : json['seed'],
        'stream': json['stream'] == null ? undefined : json['stream'],
        'temperature': json['temperature'] == null ? undefined : json['temperature'],
        'tools': json['tools'] == null ? undefined : json['tools'],
        'max_tool_calls': json['max_tool_calls'] == null ? undefined : json['max_tool_calls'],
        'parallel_tool_calls': json['parallel_tool_calls'] == null ? undefined : json['parallel_tool_calls'],
        'tool_choice': json['tool_choice'] == null ? undefined : ChatCompletionsRequestToolChoiceFromJSON(json['tool_choice']),
        'logprobs': json['logprobs'] == null ? undefined : json['logprobs'],
        'top_logprobs': json['top_logprobs'] == null ? undefined : json['top_logprobs'],
        'top_p': json['top_p'] == null ? undefined : json['top_p'],
        'usage': json['usage'] == null ? undefined : json['usage'],
    };
}

export function ChatCompletionsRequestToJSON(json: any): ChatCompletionsRequest {
    return ChatCompletionsRequestToJSONTyped(json, false);
}

export function ChatCompletionsRequestToJSONTyped(value?: ChatCompletionsRequest | null, ignoreDiscriminator: boolean = false): any {
    if (value == null) {
        return value;
    }

    return {
        
        'reasoning': ChatCompletionsRequestReasoningToJSON(value['reasoning']),
        'frequency_penalty': value['frequency_penalty'],
        'logit_bias': value['logit_bias'],
        'max_output_tokens': value['max_output_tokens'],
        'max_completions_tokens': value['max_completions_tokens'],
        'meta': value['meta'],
        'model': value['model'],
        'messages': ((value['messages'] as Array<any>).map(ChatMessageToJSON)),
        'presence_penalty': value['presence_penalty'],
        'seed': value['seed'],
        'stream': value['stream'],
        'temperature': value['temperature'],
        'tools': value['tools'],
        'max_tool_calls': value['max_tool_calls'],
        'parallel_tool_calls': value['parallel_tool_calls'],
        'tool_choice': ChatCompletionsRequestToolChoiceToJSON(value['tool_choice']),
        'logprobs': value['logprobs'],
        'top_logprobs': value['top_logprobs'],
        'top_p': value['top_p'],
        'usage': value['usage'],
    };
}

