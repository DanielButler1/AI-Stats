/*
 * AI Stats Gateway API
 *
 * Programmatic access to the AI Stats AI Gateway. All endpoints forward requests to the best available provider for the specified model, and return normalised responses with clear structured metadata.
 *
 * The version of the OpenAPI document: 0.1.0
 * 
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// GatewayUsage : Token accounting for the request where provided by the upstream model.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct GatewayUsage {
    #[serde(rename = "total_tokens", skip_serializing_if = "Option::is_none")]
    pub total_tokens: Option<i32>,
    #[serde(rename = "input_text_tokens", skip_serializing_if = "Option::is_none")]
    pub input_text_tokens: Option<i32>,
    #[serde(rename = "output_text_tokens", skip_serializing_if = "Option::is_none")]
    pub output_text_tokens: Option<i32>,
    #[serde(rename = "reasoning_tokens", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub reasoning_tokens: Option<Option<i32>>,
    #[serde(rename = "cached_read_text_tokens", default, with = "::serde_with::rust::double_option", skip_serializing_if = "Option::is_none")]
    pub cached_read_text_tokens: Option<Option<i32>>,
}

impl GatewayUsage {
    /// Token accounting for the request where provided by the upstream model.
    pub fn new() -> GatewayUsage {
        GatewayUsage {
            total_tokens: None,
            input_text_tokens: None,
            output_text_tokens: None,
            reasoning_tokens: None,
            cached_read_text_tokens: None,
        }
    }
}

