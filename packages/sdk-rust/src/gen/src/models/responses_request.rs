/*
 * AI Stats Gateway API
 *
 * A gateway API for accessing various AI models with OpenAI-compatible endpoints.
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct ResponsesRequest {
    #[serde(rename = "model")]
    pub model: String,
    #[serde(rename = "input", skip_serializing_if = "Option::is_none")]
    pub input: Option<serde_json::Value>,
    #[serde(rename = "input_items", skip_serializing_if = "Option::is_none")]
    pub input_items: Option<Vec<serde_json::Value>>,
    #[serde(rename = "conversation", skip_serializing_if = "Option::is_none")]
    pub conversation: Option<Box<models::ChatCompletionsRequestToolChoice>>,
    #[serde(rename = "include", skip_serializing_if = "Option::is_none")]
    pub include: Option<Vec<String>>,
    #[serde(rename = "instructions", skip_serializing_if = "Option::is_none")]
    pub instructions: Option<String>,
    #[serde(rename = "max_output_tokens", skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<i32>,
    #[serde(rename = "max_tool_calls", skip_serializing_if = "Option::is_none")]
    pub max_tool_calls: Option<i32>,
    #[serde(rename = "metadata", skip_serializing_if = "Option::is_none")]
    pub metadata: Option<std::collections::HashMap<String, String>>,
    #[serde(rename = "parallel_tool_calls", skip_serializing_if = "Option::is_none")]
    pub parallel_tool_calls: Option<bool>,
    #[serde(rename = "previous_response_id", skip_serializing_if = "Option::is_none")]
    pub previous_response_id: Option<String>,
    #[serde(rename = "prompt", skip_serializing_if = "Option::is_none")]
    pub prompt: Option<Box<models::ResponsesRequestPrompt>>,
    #[serde(rename = "prompt_cache_key", skip_serializing_if = "Option::is_none")]
    pub prompt_cache_key: Option<String>,
    #[serde(rename = "prompt_cache_retention", skip_serializing_if = "Option::is_none")]
    pub prompt_cache_retention: Option<String>,
    #[serde(rename = "reasoning", skip_serializing_if = "Option::is_none")]
    pub reasoning: Option<Box<models::ResponsesRequestReasoning>>,
    #[serde(rename = "safety_identifier", skip_serializing_if = "Option::is_none")]
    pub safety_identifier: Option<String>,
    #[serde(rename = "service_tier", skip_serializing_if = "Option::is_none")]
    pub service_tier: Option<String>,
    #[serde(rename = "store", skip_serializing_if = "Option::is_none")]
    pub store: Option<bool>,
    #[serde(rename = "stream", skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    #[serde(rename = "stream_options", skip_serializing_if = "Option::is_none")]
    pub stream_options: Option<serde_json::Value>,
    #[serde(rename = "temperature", skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f64>,
    #[serde(rename = "text", skip_serializing_if = "Option::is_none")]
    pub text: Option<serde_json::Value>,
    #[serde(rename = "tool_choice", skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<Box<models::ChatCompletionsRequestToolChoice>>,
    #[serde(rename = "tools", skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<serde_json::Value>>,
    #[serde(rename = "top_logprobs", skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<i32>,
    #[serde(rename = "top_p", skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f64>,
    #[serde(rename = "truncation", skip_serializing_if = "Option::is_none")]
    pub truncation: Option<String>,
    #[serde(rename = "background", skip_serializing_if = "Option::is_none")]
    pub background: Option<bool>,
    #[serde(rename = "user", skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,
    #[serde(rename = "usage", skip_serializing_if = "Option::is_none")]
    pub usage: Option<bool>,
    #[serde(rename = "meta", skip_serializing_if = "Option::is_none")]
    pub meta: Option<bool>,
}

impl ResponsesRequest {
    pub fn new(model: String) -> ResponsesRequest {
        ResponsesRequest {
            model,
            input: None,
            input_items: None,
            conversation: None,
            include: None,
            instructions: None,
            max_output_tokens: None,
            max_tool_calls: None,
            metadata: None,
            parallel_tool_calls: None,
            previous_response_id: None,
            prompt: None,
            prompt_cache_key: None,
            prompt_cache_retention: None,
            reasoning: None,
            safety_identifier: None,
            service_tier: None,
            store: None,
            stream: None,
            stream_options: None,
            temperature: None,
            text: None,
            tool_choice: None,
            tools: None,
            top_logprobs: None,
            top_p: None,
            truncation: None,
            background: None,
            user: None,
            usage: None,
            meta: None,
        }
    }
}

