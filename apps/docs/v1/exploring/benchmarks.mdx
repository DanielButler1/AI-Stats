---
title: "Exploring Benchmarks"
description: "Understand how AI model benchmarks are collected, standardised, and visualised on AI Stats."
---

Benchmarks are at the heart of how AI Stats measures model quality and progress.  
They provide **objective, repeatable metrics** that help you compare model performance across tasks, domains, and providers â€” from reasoning and coding to language understanding and image recognition.

---

## What are benchmarks?

A **benchmark** is a dataset or evaluation designed to test a modelâ€™s performance on a specific task.  
By running multiple models on the same dataset, we can measure relative strengths and weaknesses.

Common examples include:

-   ðŸ§  **MMLU (Massive Multitask Language Understanding)** â€” tests general knowledge and reasoning.
-   ðŸ”¢ **GSM8K** â€” tests mathematical reasoning and problem-solving.
-   ðŸ’¬ **HellaSwag** â€” evaluates common sense and contextual understanding.
-   ðŸ§® **ARC-Challenge** â€” measures scientific reasoning and logic.
-   ðŸ’¡ **HumanEval** â€” assesses programming and code generation skills.

---

## How AI Stats handles benchmarks

AI Stats collects benchmark data from **multiple public sources** and **official reports**, then standardises them into a consistent format.

Each modelâ€™s benchmark section includes:

-   **Score value** â€” typically expressed as a percentage or normalised scale.
-   **Dataset source** â€” e.g. MMLU, GSM8K, etc.
-   **Evaluation method** â€” indicates how the score was derived (official report, third-party eval, or community submission).
-   **Last updated** â€” shows when the benchmark data was last refreshed.

---

## Example benchmark page

---

## Benchmark categories

Benchmarks are grouped into categories that represent the skill being tested:

| Category                   | Examples               | Description                                                           |
| -------------------------- | ---------------------- | --------------------------------------------------------------------- |
| **Reasoning**              | MMLU, GSM8K, ARC       | Tests general logic, multi-step reasoning, and problem-solving.       |
| **Language Understanding** | HellaSwag, BoolQ, PIQA | Measures reading comprehension, common sense, and linguistic ability. |
| **Code Generation**        | HumanEval, MBPP        | Evaluates programming and code synthesis accuracy.                    |
| **Math & Science**         | GSM8K, MATH, SciQ      | Focuses on numerical and scientific reasoning.                        |
| **Multimodal**             | MathVista, MMMU        | Combines text and visual comprehension tasks.                         |

---

## How to interpret scores

| Value Range | Interpretation                                                    |
| ----------- | ----------------------------------------------------------------- |
| **90-100%** | Exceptional â€” near state-of-the-art performance.                  |
| **80-89%**  | Excellent â€” capable for most production use cases.                |
| **60-79%**  | Good â€” reliable for general tasks but may struggle on edge cases. |
| **< 60%**   | Developing â€” best for experimental or research scenarios.         |

Keep in mind that some benchmarks have **different scoring systems**.  
AI Stats normalises these where possible to make comparisons more meaningful.

---

## Visualising benchmarks on AI Stats

Benchmark results are visualised using:

-   ðŸ“Š **Bar charts** for comparing models within the same dataset.
-   ðŸ“ˆ **Trend charts** showing model improvements over time.
-   ðŸ§® **Aggregate scores** to summarise performance across multiple datasets.

Each benchmark dataset on AI Stats includes filters for:

-   Model type (chat, code, embedding, etc.)
-   Provider
-   Release year
-   Dataset category

This lets you quickly identify patterns â€” like which families dominate reasoning or coding tasks.

---

## Data sources and methodology

AI Stats aggregates benchmark data from:

-   Official research papers and model reports.
-   Provider evaluation pages (OpenAI Evals, Anthropic Reports, etc.).
-   Open community datasets (e.g. Hugging Face leaderboards).
-   Direct submissions from users via GitHub.

All data is standardised and verified before being published.  
See [Research & Methodology â†’ Benchmark Methodology](/v1/research/benchmark-methodology) for full details.

---

## Example use cases

| Goal                                 | Example                                                              |
| ------------------------------------ | -------------------------------------------------------------------- |
| Compare reasoning models             | â€œWhich model scores highest on GSM8K?â€                               |
| Evaluate cost-performance trade-offs | â€œDoes GPT-4o justify its higher cost compared to Claude 3.5 Sonnet?â€ |
| Track yearly progress                | â€œHow much has MMLU accuracy improved since 2022?â€                    |
| Research benchmarks by category      | â€œWhich models lead in multimodal reasoning?â€                         |

---

## Contributing benchmark data

You can submit new or corrected benchmark scores to help keep AI Stats up to date.  
Each submission is reviewed before inclusion to ensure consistency and accuracy.

<Card
	title="Contribute Benchmark Data"
	icon="github"
	href="/v1/contributing/editing-a-benchmark"
	horizontal
>
	Learn how to add or edit benchmark information safely.
</Card>

---

## Next steps

Once you understand how benchmark data works, you can explore the **API providers** that make these models available programmatically.

<Card
	title="Explore API Providers"
	icon="network"
	href="/v1/exploring/api-providers"
	horizontal
>
	See where and how models are hosted, priced, and accessed.
</Card>
